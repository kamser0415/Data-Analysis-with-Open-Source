{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamser0415/Data-Analysis-with-Open-Source/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_14%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14강 비정형 데이터 분석 : 패션 사진 데이터 활용\n",
        "\n",
        "### 목표\n",
        "\n",
        "- 비정형 데이터를 인공지능 모델로 분석하여 실무에서 활용 가능한 보고서 형태로 가공\n",
        "\n",
        "- 패션 트렌드라는 구체적인 주제를 통해, 비정형 데이터 분석의 실질적인 활용 방안을 경험하고자 함\n",
        "\n",
        "\n",
        "### 분석 프로세스 개요\n",
        "\n",
        "1. 데이터 수집\n",
        "  - requests를 이용한 RSS 데이터 수집\n",
        "  - lxml을 이용한 XML 파싱\n",
        "  - 이미지 데이터 추출\n",
        "2. VLM을 이용한 이미지 분석\n",
        "  - 프롬프트를 이용한 이미지 필터링\n",
        "  - 프롬프트를 이용한 스타일 분석\n",
        "3. LLM을 이용한 키워드 분석 및 보고서 작성\n",
        "  - 텍스트 전처리\n",
        "  - 색상 및 스타일 키워드 추출\n",
        "  - 워드 클라우드 분석\n",
        "  - 보고서 작성\n",
        "\n",
        "# 주의 : 런타임 GPU 로 설정 필요"
      ],
      "metadata": {
        "id": "xFHAUHwUwqEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4bit VLM 처리를 위한 bitsandbytes 설치\n",
        "# LLM 처리를 위한 VLLM 설치 (오래걸리는 작업(>5분)이므로 미리 실행!)\n",
        "!pip install bitsandbytes==0.45.3 vllm==0.7.3 transformers==4.48.2\n",
        "# 필요 시 세션 재시작"
      ],
      "metadata": {
        "id": "oRFE0WufwtWm",
        "collapsed": true,
        "outputId": "828cc452-b77f-4e9c-b6ec-e9ecb88f77b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.40.0-cp312-cp312-manylinux2014_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m593.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.11-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m808.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
            "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.16-py3-none-any.whl (12 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20250909-py3-none-any.whl (914 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.4/914.4 kB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.16.0-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastar-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (820 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.6/820.6 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, triton, sympy, rignore, pycountry, pybind11, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, msgspec, lark, interegular, httptools, fastar, dnspython, diskcache, blake3, astor, airportsdata, watchfiles, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, email-validator, depyf, tokenizers, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, transformers, torch, ray, outlines_core, fastapi-cloud-cli, fastapi-cli, xgrammar, xformers, torchvision, torchaudio, outlines, mistral_common, compressed-tensors, bitsandbytes, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20250909 astor-0.8.1 bitsandbytes-0.45.3 blake3-1.0.8 compressed-tensors-0.9.1 depyf-0.18.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.16 fastapi-cloud-cli-0.5.2 fastar-0.7.0 gguf-0.10.0 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 lm-format-enforcer-0.10.12 mistral_common-1.8.5 msgspec-0.20.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybind11-3.0.1 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.40.0 rich-toolkit-0.16.0 rignore-0.7.6 sympy-1.13.1 tokenizers-0.21.4 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 transformers-4.48.2 triton-3.1.0 uvloop-0.22.1 vllm-0.7.3 watchfiles-1.1.1 xformers-0.0.28.post3 xgrammar-0.1.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e6693a0b458e4f92b8056425ac26750d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 처리를 위한 matplotlib 설정 (1)\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache –fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "2PyUVMjnwv4n",
        "outputId": "213e73bb-84b2-4aa2-b3d8-e7493de9c086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 2s (4,205 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 런타임 -> 세션 다시 시작"
      ],
      "metadata": {
        "id": "xjT6NMJ_wxoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 처리를 위한 matplotlib 설정 (2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ],
      "metadata": {
        "id": "1Q-jeRwcwzly"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터 수집 및 전처리"
      ],
      "metadata": {
        "id": "Evptw6-lw2j-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-1 RSS 피드에서 이미지 URL 추출"
      ],
      "metadata": {
        "id": "GyE957VBw3Nt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjrPWcd1AFKx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from lxml import etree\n",
        "from lxml.html import fromstring\n",
        "import pandas as pd\n",
        "\n",
        "## RSS 데이터는 리치 사이트 서머리로 웹 1.0 시절에 뉴스나 잡지를 보고 싶을 때 그 사이트에 접속해서\n",
        "\n",
        "def extract_unique_images(rss_url):\n",
        "    ## 주어진 RSS 피드 URL에서 고유한 이미지 URL들을 추출하는 함수 정의\n",
        "    try:\n",
        "        ## requests 라이브러리를 사용하여 RSS 피드 URL로부터 내용을 가져옴\n",
        "        response = requests.get(rss_url)\n",
        "        ## 가져온 XML 응답 내용을 lxml의 etree.fromstring으로 파싱하여 XML 트리 root를 생성\n",
        "\n",
        "        image_urls = set()\n",
        "\n",
        "        ## XML 트리에서 모든 'item' 태그를 XPath를 사용하여 순회\n",
        "        for item in root.xpath('//item'):\n",
        "            description = item.find('description')\n",
        "            if description is not None and description.text:\n",
        "                ## description의 텍스트 내용을 lxml.html.fromstring으로 파싱하여 HTML 트리를 생성\n",
        "\n",
        "                ## HTML 트리에서 첫 번째 <img> 태그의 'src' 속성 값을 XPath를 사용하여 추출\n",
        "\n",
        "                if img_url:\n",
        "                    image_urls.add(img_url)\n",
        "\n",
        "        return list(image_urls)\n",
        "\n",
        "    except Exception as e:\n",
        "        ## 오류 발생 시 오류 메시지를 출력하고 빈 리스트를 반환\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "rss_url = \"https://glltn.com/feed/\"\n",
        "## extract_unique_images 함수를 호출하여 고유한 이미지 URL들을 추출\n",
        "unique_images = extract_unique_images(rss_url)\n",
        "\n",
        "## 추출된 이미지 URL 리스트를 사용하여 'image'라는 열을 가진 pandas DataFrame을 생성\n",
        "df = pd.DataFrame(unique_images, columns=[\"image\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-2 수집 데이터 확인"
      ],
      "metadata": {
        "id": "XT6FFqgn42yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def path_to_image_html(path):\n",
        "    ## 이미지 경로를 HTML img 태그로 변환하는 함수\n",
        "    return f'<img src=\"{path}\" width=\"300\" />'\n",
        "\n",
        "## DataFrame의 스타일을 설정하여 이미지 너비를 300px로 지정\n",
        "df.style.set_table_styles([{'selector': 'img', 'props': 'width: 300px;'}])\n",
        "\n",
        "## DataFrame을 HTML로 변환하여 출력. 이미지 열은 path_to_image_html 함수로 포맷팅\n",
        "display(HTML(df.to_html(escape=False, formatters=dict(**{'image': path_to_image_html}))))"
      ],
      "metadata": {
        "id": "DVMvyxJOAOo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. VLM을 이용한 이미지 분석"
      ],
      "metadata": {
        "id": "X8tIpX9A3oMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-3 VLM 모델 로드"
      ],
      "metadata": {
        "id": "wKQ3p2-alzLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "## 'openbmb/MiniCPM-V-2_6-int4' 모델을 사전 훈련된 가중치와 함께 로드\n",
        "## trust_remote_code=True는 허브에서 사용자 정의 코드를 실행할 수 있도록 허용\n",
        "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6-int4', trust_remote_code=True)\n",
        "## 로드된 모델에 해당하는 토크나이저를 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6-int4', trust_remote_code=True)\n",
        "## 모델을 평가 모드로 설정 (드롭아웃 등 훈련 시에만 필요한 기능 비활성화)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "c1IXBK01ASWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://farm3.staticflickr.com/2677/4434956914_6e95a22940_z.jpg)"
      ],
      "metadata": {
        "id": "xrr3p8fslk4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-4 이미지 질문 응답 예시"
      ],
      "metadata": {
        "id": "58PxIp8SmUl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "## 재현성을 위해 시드(seed)를 42로 설정\n",
        "set_seed(42)\n",
        "## 예시 이미지 URL 정의\n",
        "image_url = 'https://farm3.staticflickr.com/2677/4434956914_6e95a22940_z.jpg'\n",
        "## requests로 이미지 다운로드 후 PIL Image 객체로 열고 RGB 형식으로 변환\n",
        "image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
        "## 이미지에 대한 질문 정의\n",
        "question = 'how many cats in the photo?'\n",
        "## 모델 입력 형식에 맞춰 메시지 구성 (이미지와 질문 포함)\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "## 모델의 chat 함수를 호출하여 이미지와 질문에 대한 응답 생성\n",
        "result = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)\n",
        "## 모델의 응답 출력\n",
        "print(result)"
      ],
      "metadata": {
        "id": "OLk-R6PYATVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "## 이미지에 대한 질문을 업데이트. 책 표지의 고양이도 포함하도록 요청\n",
        "question = 'how many cats in the photo? including the books cover.'\n",
        "## 모델 입력 형식에 맞춰 메시지 구성 (이전에 로드된 이미지와 업데이트된 질문 포함)\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "## 모델의 chat 함수를 호출하여 업데이트된 질문에 대한 응답 생성\n",
        "result = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)\n",
        "## 모델의 응답 출력\n",
        "print(result)"
      ],
      "metadata": {
        "id": "B1KelJIrAU2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "## 이미지에 대한 질문을 'describe the photo'로 설정하여 이미지 내용을 설명하도록 요청\n",
        "question = 'describe the photo'\n",
        "## 모델 입력 형식에 맞춰 메시지 구성 (이전에 로드된 이미지와 설명 요청 질문 포함)\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "## 모델의 chat 함수를 호출하여 이미지에 대한 설명을 생성\n",
        "result = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)\n",
        "## 모델의 응답 (이미지 설명) 출력\n",
        "print(result)"
      ],
      "metadata": {
        "id": "UCd9smsCAWH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-5 의류 이미지 여부 판단"
      ],
      "metadata": {
        "id": "Hyv4G27NnxxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_picture_of_clothing(image_url):\n",
        "    ## 이미지 URL이 의류 사진인지 판단하는 함수\n",
        "    # 의류가 포함된 사진인지 확인하는 질문 작성 (영어로)\n",
        "    question = ''\n",
        "    image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
        "    msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "    result = model.chat(image=None, msgs=msgs, tokenizer=tokenizer, temperature=0.1)\n",
        "    print(result)\n",
        "    ## 응답에 'yes'가 포함되어 있는지 확인하여 True/False 반환\n",
        "    return 'yes' in result.lower()\n",
        "\n",
        "## DataFrame의 'image' 열에 함수를 적용하여 'is_clothing' 열에 결과 저장\n",
        "df['is_clothing'] = df['image'].apply(is_picture_of_clothing)"
      ],
      "metadata": {
        "id": "KnJ0trHgAXTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-6 의류 판단 결과 시각화"
      ],
      "metadata": {
        "id": "l4l7QsLaoCoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(df.to_html(escape=False, formatters=dict(**{'image': path_to_image_html}))))"
      ],
      "metadata": {
        "id": "rOtwke1pAd8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-7 의류 이미지 필터링"
      ],
      "metadata": {
        "id": "0aSaNQ7eoUWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 'is_clothing' 열의 값이 True인 행들만 필터링하여 DataFrame을 업데이트\n"
      ],
      "metadata": {
        "id": "X3prbhccAfdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-8 의류 스타일 분석"
      ],
      "metadata": {
        "id": "UMo5LkOzoaEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_style(image_url):\n",
        "    ## 주어진 이미지 URL의 의류 스타일을 분석하는 함수\n",
        "    question = ''\n",
        "    image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
        "    msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "    ## 모델의 chat 함수를 호출하여 이미지에 대한 스타일 분석 응답 생성\n",
        "    result = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)\n",
        "    return result\n",
        "\n",
        "## 필터링된 DataFrame의 'image' 열에 describe_style 함수를 적용\n",
        "## 결과는 'style'이라는 새로운 열에 저장\n",
        "df['style'] = df['image'].apply(describe_style)"
      ],
      "metadata": {
        "id": "FxgObQ1ZAgVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(df.to_html(escape=False, formatters=dict(**{'image': path_to_image_html}))))"
      ],
      "metadata": {
        "id": "i-znn3Z0qjJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LLM을 이용한 키워드 분석 및 보고서 작성"
      ],
      "metadata": {
        "id": "0z6BhTZeopo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-9 언어 모델(LLM) 로드"
      ],
      "metadata": {
        "id": "ebRE0K15oqr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "## vLLM 라이브러리를 사용하여 'LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct' 모델을 로드\n",
        "## gpu_memory_utilization은 GPU 메모리 사용 비율을 0.5로 설정\n",
        "## max_model_len은 모델이 처리할 수 있는 최대 토큰 길이를 10000으로 설정\n"
      ],
      "metadata": {
        "id": "hiQAZ-csAiI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-10 색상 정보 추출"
      ],
      "metadata": {
        "id": "Sj470n8wo86w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams ## SamplingParams 임포트가 필요\n",
        "\n",
        "def extract_color(style):\n",
        "  ## 주어진 스타일 설명 텍스트에서 색상을 한글로 추출하는 함수\n",
        "  prompt = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\" # vlm이 작성한 글에서 색상 정보 추출, 한글로 번역하면서\n",
        "      }\n",
        "  ]\n",
        "  ## 샘플링 파라미터 설정 (온도, top_p, 최대 토큰 수)\n",
        "  sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=1024)\n",
        "  ## LLM 모델을 사용하여 프롬프트에 대한 응답 생성\n",
        "  result = llm.chat(prompt, sampling_params)[0].outputs[0].text\n",
        "  print(result)\n",
        "  return result\n",
        "\n",
        "## DataFrame의 'style' 열에 extract_color 함수를 적용\n",
        "## 결과는 'color'라는 새로운 열에 저장\n",
        "df['color'] = df['style'].apply(extract_color)"
      ],
      "metadata": {
        "id": "ci7oHspgAjuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-11 스타일 키워드 추출"
      ],
      "metadata": {
        "id": "hdHnIoYapBPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams ## SamplingParams 임포트가 필요\n",
        "\n",
        "def extract_color(style):\n",
        "  ## 주어진 스타일 설명 텍스트에서 스타일 키워드를 한글로 추출하는 함수\n",
        "  prompt = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"\" # vlm이 작성한 글에서 스타일 키워드 추출, 한글로 번역하면서\n",
        "      }\n",
        "  ]\n",
        "  ## 샘플링 파라미터 설정 (온도, top_p, 최대 토큰 수)\n",
        "  sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=1024)\n",
        "  ## LLM 모델을 사용하여 프롬프트에 대한 응답 생성\n",
        "  result = llm.chat(prompt, sampling_params)[0].outputs[0].text\n",
        "  print(result)\n",
        "  return result\n",
        "\n",
        "## DataFrame의 'style' 열에 extract_color 함수를 적용 (함수 이름은 이전과 동일하지만 기능 변경)\n",
        "## 결과는 'keyword'라는 새로운 열에 저장\n",
        "df['keyword'] = df['style'].apply(extract_color)"
      ],
      "metadata": {
        "id": "9CGzC0QlAlNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(df.to_html(escape=False, formatters=dict(**{'image': path_to_image_html}))))"
      ],
      "metadata": {
        "id": "nxI64xM-qpRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-12 텍스트 데이터 정제"
      ],
      "metadata": {
        "id": "sLXLdNUhpK8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    ## 텍스트에서 특수 문자 및 HTML 태그를 제거하고 소문자로 변환하는 함수\n",
        "    if isinstance(text, str):\n",
        "       ## 영문, 숫자, 한글, 공백을 제외한 모든 문자 제거\n",
        "       text = re.sub(r'[^a-zA-Z0-9가-힣\\s]', '', text)\n",
        "       ## HTML 태그 제거\n",
        "       text = re.sub(r'<[^>]*>', '', text)\n",
        "       ## 텍스트를 소문자로 변환\n",
        "       text = text.lower()\n",
        "       return text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "## 'color' 열의 텍스트 데이터 정제\n",
        "df['color'] = df['color'].apply(clean_text)\n",
        "## 'keyword' 열의 텍스트 데이터 정제\n",
        "df['keyword'] = df['keyword'].apply(clean_text)"
      ],
      "metadata": {
        "id": "nAhvdWHVAm0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-13 워드 클라우드 생성 및 시각화"
      ],
      "metadata": {
        "id": "CNg6DakapO_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_word_count(df):\n",
        "    ## DataFrame의 'color'와 'keyword' 열에서 단어 빈도를 계산하는 함수\n",
        "    if not df.empty:\n",
        "        ## 'color' 열의 모든 단어를 리스트로 합침\n",
        "        all_nouns = df['color'].apply(str.split).sum()\n",
        "        ## 'keyword' 열의 모든 단어를 추가\n",
        "        all_nouns += df['keyword'].apply(str.split).sum()\n",
        "        ## '색상' 단어를 제외한 모든 단어를 필터링\n",
        "        all_nouns = [word for word in all_nouns if word not in ['색상']]\n",
        "        ## 단어 빈도를 Counter 객체로 반환\n",
        "        return Counter(all_nouns)\n",
        "    return Counter() ## DataFrame이 비어있으면 빈 Counter 반환\n",
        "\n",
        "def create_wordcloud(word_count):\n",
        "    ## 단어 빈도수를 기반으로 워드 클라우드를 생성하고 시각화하는 함수\n",
        "    if not word_count: ## 단어 빈도가 없으면 워드클라우드 생성하지 않음\n",
        "        print(\"No words to generate word cloud.\")\n",
        "        return\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white',\n",
        "        colormap='viridis',\n",
        "        font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf' ## 한글 폰트 경로 지정\n",
        "        ).generate_from_frequencies(word_count)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\") ## 축 표시 제거\n",
        "    plt.show() ## 워드 클라우드 출력\n",
        "\n",
        "## DataFrame에서 단어 빈도 계산\n",
        "word_count = get_word_count(df)\n",
        "## 계산된 단어 빈도로 워드 클라우드 생성 및 시각화\n",
        "create_wordcloud(word_count)"
      ],
      "metadata": {
        "id": "newpDQHoAoGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-14 트렌드 분석 보고서 생성 프롬프트 구성 및 실행"
      ],
      "metadata": {
        "id": "eDO_o8uop2M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14-15 분석 보고서 시각화"
      ],
      "metadata": {
        "id": "m304vCZMp7ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams ## SamplingParams 임포트가 필요\n",
        "\n",
        "## 시스템 메시지로 시작하는 프롬프트 리스트 초기화\n",
        "prompt = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"\n",
        "    }\n",
        "]\n",
        "## DataFrame의 각 행을 순회하며 '스타일 노트'와 '이미지 URL'을 사용자 메시지로 추가\n",
        "for row in df.itertuples():\n",
        "  prompt.append({\"role\": \"user\", \"content\": f\"\"})\n",
        "## 마지막으로, 종합적인 트렌드 분석 보고서 작성을 요청하는 사용자 메시지 추가\n",
        "## 보고서 제목, 내용의 전문성, 마크다운 형식, 예시 이미지 포함을 지시\n",
        "prompt.append({\"role\": \"user\", \"content\": \"\"})\n",
        "\n",
        "## 샘플링 파라미터 설정 (온도, top_p, 최대 토큰 수)\n",
        "sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=4096)\n",
        "## LLM 모델을 사용하여 구성된 프롬프트에 대한 응답 생성\n",
        "result = llm.chat(prompt, sampling_params)[0].outputs[0].text"
      ],
      "metadata": {
        "id": "x6LZ3FCtApst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "## LLM으로부터 생성된 결과(Markdown 형식의 보고서)를 Jupyter 환경에 표시\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "7C7KDpS6Aq7w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}